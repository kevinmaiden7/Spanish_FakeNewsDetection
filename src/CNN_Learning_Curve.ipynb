{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_preprocessing import get_input_share_tokenizer\n",
    "from pretrained_embedding import get_input_plus_embedding_vectors_share_tokenizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_length, max_length_sequence, emb_dim, transfer_learning, embedding_vectors,\n",
    "                 filters, kernel_size, dense_units, l2_kernel):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ))\n",
    "    \n",
    "    if transfer_learning:\n",
    "        embedding_layer = Embedding(input_dim = vocabulary_length, output_dim = emb_dim, weights=[embedding_vectors],\n",
    "                                trainable = False)(X_input)\n",
    "    else:\n",
    "        embedding_layer = Embedding(input_dim = vocabulary_length, output_dim = emb_dim,\n",
    "                                trainable = True)(X_input)\n",
    "    \n",
    "    X = Conv1D(filters = filters, kernel_size = kernel_size, activation = 'relu',\n",
    "              kernel_regularizer = regularizers.l2(l2_kernel))(embedding_layer)\n",
    "    X = MaxPooling1D(pool_size = 2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units = dense_units, activation = 'relu')(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add n_samples from data_2 to data_1\n",
    "def add_data_portion(data_1, data_2, n_samples):\n",
    "    df_slice = data_2.sample(n_samples)\n",
    "    df_rest = data_2.loc[~data_2.index.isin(df_slice.index)]\n",
    "    df_extended = data_1.append(df_slice, ignore_index = True)\n",
    "    \n",
    "    df_rest.reset_index(inplace = True)\n",
    "    del(df_rest['index'])\n",
    "    df_extended.reset_index(inplace = True)\n",
    "    del(df_extended['index'])\n",
    "    \n",
    "    return df_extended, df_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51228</th>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51229</th>\n",
       "      <td>The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51230</th>\n",
       "      <td>Anti-Trump Protesters Are Tools of the Oligar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51231</th>\n",
       "      <td>ADDIS ABABA, Ethiopia —President Obama convene...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51232</th>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51233 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Donald Trump just couldn t wish all Americans ...      1\n",
       "1      House Intelligence Committee Chairman Devin Nu...      1\n",
       "2      On Friday, it was revealed that former Milwauk...      1\n",
       "3      On Christmas day, Donald Trump announced that ...      1\n",
       "4      Pope Francis used his annual Christmas Day mes...      1\n",
       "...                                                  ...    ...\n",
       "51228  The State Department told the Republican Natio...      0\n",
       "51229  The ‘P’ in PBS Should Stand for ‘Plutocratic’ ...      1\n",
       "51230   Anti-Trump Protesters Are Tools of the Oligar...      1\n",
       "51231  ADDIS ABABA, Ethiopia —President Obama convene...      0\n",
       "51232  Jeb Bush Is Suddenly Attacking Trump. Here's W...      0\n",
       "\n",
       "[51233 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_dataset = pd.read_csv('../data/Merged/english_dataset.csv')\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RAE WILL INCLUDE THE WORD \"LADY\" IN THE SPANIS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The word \"haiga\", accepted by the RAE The Roya...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YORDI ROSADO WILL WRITE AND DESIGN THE NEW SEP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNAM will train teachers to pass the Pisa test...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alert: they intend to approve school books wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>We recover the story of Aleixandra, the 21-yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>Reproaches, tension and sincerity: the meal in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>RT @ElMundoOpinion: \"PSOE, PP, Ciudadanos and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>Russia quotes the Spanish ambassador for some ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>Saeed Malekpour was arrested in 2008, while vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     RAE WILL INCLUDE THE WORD \"LADY\" IN THE SPANIS...      1\n",
       "1     The word \"haiga\", accepted by the RAE The Roya...      1\n",
       "2     YORDI ROSADO WILL WRITE AND DESIGN THE NEW SEP...      1\n",
       "3     UNAM will train teachers to pass the Pisa test...      0\n",
       "4     Alert: they intend to approve school books wit...      1\n",
       "...                                                 ...    ...\n",
       "2566  We recover the story of Aleixandra, the 21-yea...      0\n",
       "2567  Reproaches, tension and sincerity: the meal in...      0\n",
       "2568  RT @ElMundoOpinion: \"PSOE, PP, Ciudadanos and ...      0\n",
       "2569  Russia quotes the Spanish ambassador for some ...      0\n",
       "2570  Saeed Malekpour was arrested in 2008, while vi...      0\n",
       "\n",
       "[2571 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_dataset = pd.read_csv('../data/Merged/spanish_t_dataset.csv')\n",
    "translated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_length = 10000\n",
    "max_length_sequence = 1500\n",
    "emb_dim = 300\n",
    "language = 'english'\n",
    "embedding_file_path = '../data/GloVe_Embedding/glove.6B.300d.txt'\n",
    "epochs = 7\n",
    "batch_size = 32\n",
    "\n",
    "slice_size = [500, 1000, 1500, 2000, 2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve with trainable embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51733, 1500) (2071, 1500)\n",
      "Epoch 1/7\n",
      "51733/51733 [==============================] - 1652s 32ms/step - loss: 0.1003 - accuracy: 0.9577\n",
      "Epoch 2/7\n",
      "51733/51733 [==============================] - 1681s 32ms/step - loss: 0.0251 - accuracy: 0.9919\n",
      "Epoch 3/7\n",
      "51733/51733 [==============================] - 1714s 33ms/step - loss: 0.0075 - accuracy: 0.9977\n",
      "Epoch 4/7\n",
      "51733/51733 [==============================] - 1590s 31ms/step - loss: 0.0053 - accuracy: 0.9983\n",
      "Epoch 5/7\n",
      "51733/51733 [==============================] - 2109s 41ms/step - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 6/7\n",
      "51733/51733 [==============================] - 1331s 26ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 7/7\n",
      "51733/51733 [==============================] - 1121s 22ms/step - loss: 0.0031 - accuracy: 0.9992\n",
      "2071/2071 [==============================] - 10s 5ms/step\n",
      "0.611\n",
      "(52233, 1500) (1571, 1500)\n",
      "Epoch 1/7\n",
      "52233/52233 [==============================] - 1471s 28ms/step - loss: 0.1110 - accuracy: 0.9579\n",
      "Epoch 2/7\n",
      "52233/52233 [==============================] - 1370s 26ms/step - loss: 0.0278 - accuracy: 0.9913\n",
      "Epoch 3/7\n",
      "52233/52233 [==============================] - 1184s 23ms/step - loss: 0.0096 - accuracy: 0.9973\n",
      "Epoch 4/7\n",
      "52233/52233 [==============================] - 1120s 21ms/step - loss: 0.0051 - accuracy: 0.9983\n",
      "Epoch 5/7\n",
      "52233/52233 [==============================] - 1126s 22ms/step - loss: 0.0069 - accuracy: 0.9979\n",
      "Epoch 6/7\n",
      "52233/52233 [==============================] - 1124s 22ms/step - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 7/7\n",
      "52233/52233 [==============================] - 1119s 21ms/step - loss: 0.0065 - accuracy: 0.9986\n",
      "1571/1571 [==============================] - 7s 4ms/step\n",
      "0.642\n",
      "(52733, 1500) (1071, 1500)\n",
      "Epoch 1/7\n",
      "52733/52733 [==============================] - 1129s 21ms/step - loss: 0.1060 - accuracy: 0.9607\n",
      "Epoch 2/7\n",
      "52733/52733 [==============================] - 1127s 21ms/step - loss: 0.0285 - accuracy: 0.9907\n",
      "Epoch 3/7\n",
      "52733/52733 [==============================] - 1125s 21ms/step - loss: 0.0114 - accuracy: 0.9969\n",
      "Epoch 4/7\n",
      "52733/52733 [==============================] - 1126s 21ms/step - loss: 0.0074 - accuracy: 0.9977\n",
      "Epoch 5/7\n",
      "52733/52733 [==============================] - 1121s 21ms/step - loss: 0.0058 - accuracy: 0.9984\n",
      "Epoch 6/7\n",
      "52733/52733 [==============================] - 1121s 21ms/step - loss: 0.0042 - accuracy: 0.9987\n",
      "Epoch 7/7\n",
      "52733/52733 [==============================] - 1121s 21ms/step - loss: 0.0054 - accuracy: 0.9985\n",
      "1071/1071 [==============================] - 5s 4ms/step\n",
      "0.638\n",
      "(53233, 1500) (571, 1500)\n",
      "Epoch 1/7\n",
      "53233/53233 [==============================] - 1142s 21ms/step - loss: 0.1122 - accuracy: 0.9531\n",
      "Epoch 2/7\n",
      "53233/53233 [==============================] - 1142s 21ms/step - loss: 0.0327 - accuracy: 0.9889\n",
      "Epoch 3/7\n",
      "53233/53233 [==============================] - 1129s 21ms/step - loss: 0.0097 - accuracy: 0.9971\n",
      "Epoch 4/7\n",
      "53233/53233 [==============================] - 1202s 23ms/step - loss: 0.0066 - accuracy: 0.9976\n",
      "Epoch 5/7\n",
      "53233/53233 [==============================] - 1161s 22ms/step - loss: 0.0056 - accuracy: 0.9984\n",
      "Epoch 6/7\n",
      "53233/53233 [==============================] - 1161s 22ms/step - loss: 0.0073 - accuracy: 0.9983\n",
      "Epoch 7/7\n",
      "53233/53233 [==============================] - 1180s 22ms/step - loss: 0.0078 - accuracy: 0.9982\n",
      "571/571 [==============================] - 2s 4ms/step\n",
      "0.648\n",
      "(53733, 1500) (71, 1500)\n",
      "Epoch 1/7\n",
      "53733/53733 [==============================] - 1161s 22ms/step - loss: 0.2776 - accuracy: 0.9426\n",
      "Epoch 2/7\n",
      "53733/53733 [==============================] - 1177s 22ms/step - loss: 0.1137 - accuracy: 0.9818\n",
      "Epoch 3/7\n",
      "53733/53733 [==============================] - 1150s 21ms/step - loss: 0.0560 - accuracy: 0.9916\n",
      "Epoch 4/7\n",
      "53733/53733 [==============================] - 1146s 21ms/step - loss: 0.0345 - accuracy: 0.9940\n",
      "Epoch 5/7\n",
      "53733/53733 [==============================] - 1149s 21ms/step - loss: 0.0257 - accuracy: 0.9951\n",
      "Epoch 6/7\n",
      "53733/53733 [==============================] - 1167s 22ms/step - loss: 0.0195 - accuracy: 0.9965\n",
      "Epoch 7/7\n",
      "53733/53733 [==============================] - 1148s 21ms/step - loss: 0.0186 - accuracy: 0.9967\n",
      "71/71 [==============================] - 0s 6ms/step\n",
      "0.732\n",
      "[0.611, 0.642, 0.638, 0.648, 0.732]\n"
     ]
    }
   ],
   "source": [
    "trainable_emb_results = []\n",
    "\n",
    "for n_samples in slice_size:\n",
    "    \n",
    "    train_dataset, test_dataset = add_data_portion(english_dataset, translated_dataset, n_samples) # add n_samples from data_2 to data_1\n",
    "    \n",
    "    X_train, X_test, df1, df2 = get_input_share_tokenizer(train_dataset, test_dataset, stemming = False, remove_stopwords = True, \n",
    "                                                          vocabulary_length = vocabulary_length, max_length_sequence = max_length_sequence, language = language)\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    model = create_model(vocabulary_length, max_length_sequence, emb_dim, transfer_learning = False, embedding_vectors = False,\n",
    "                         filters = 16, kernel_size = 10, dense_units = 12, l2_kernel = 0)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    \n",
    "    Y_train = train_dataset.label.values\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    Y_test = test_dataset.label.values\n",
    "    loss, acc = model.evaluate(X_test, Y_test)\n",
    "    trainable_emb_results.append(round(acc, 3))\n",
    "    print(round(acc, 3))\n",
    "    \n",
    "print(trainable_emb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve with fixed embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_emb_results = []\n",
    "\n",
    "for n_samples in slice_size:\n",
    "    \n",
    "    train_dataset, test_dataset = add_data_portion(english_dataset, translated_dataset, n_samples) # add n_samples from data_2 to data_1\n",
    "    \n",
    "    X_train, X_test, embedding_vectors = get_input_plus_embedding_vectors_share_tokenizer(train_dataset, test_dataset, embedding_file_path, \n",
    "                                      vocabulary_length, max_length_sequence, emb_dim, language)\n",
    "    print(X_train.shape, X_test.shape, embedding_vectors.shape)\n",
    "    \n",
    "    model = create_model(vocabulary_length, max_length_sequence, emb_dim, transfer_learning = True, embedding_vectors = embedding_vectors, \n",
    "                         filters = 16, kernel_size = 10, dense_units = 4, l2_kernel = 0)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    \n",
    "    Y_train = train_dataset.label.values\n",
    "    model.fit(X_train, Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    Y_test = test_dataset.label.values\n",
    "    loss, acc = model.evaluate(X_test, Y_test)\n",
    "    fixed_emb_results.append(round(acc, 3))\n",
    "    print(round(acc, 3))\n",
    "    \n",
    "print(fixed_emb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(slice_size, trainable_emb_results, color = 'blue', label = 'trainable_emb_results')\n",
    "#plt.plot(slice_size, fixed_emb_results, color = 'green', label = 'fixed_emb_results')\n",
    "plt.xlabel('n_samples')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('CNN_Learning Curve')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
