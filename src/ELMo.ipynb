{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from data_preprocessing import remove_stop_words, text_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "L5a6emTisYeo",
    "outputId": "8773fd64-7cfe-42c1-c14f-80d918bcdfc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print (\"setting tensorflow version in colab\")\n",
    "    %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0Zg2jilsfC-",
    "outputId": "4754356a-786a-4e59-d1f2-5cc1e2037b5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyTRHmBZtquK",
    "outputId": "7068af51-b753-4c4f-ab49-ad2705e8909d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 2) (2571, 2)\n"
     ]
    }
   ],
   "source": [
    "english_dataset = pd.read_csv('../data/English_1/small_english_dataset.csv')\n",
    "translated_dataset = pd.read_csv('../data/Merged/spanish_t_dataset.csv')\n",
    "print(english_dataset.shape, translated_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dGEd5xksyjqH"
   },
   "outputs": [],
   "source": [
    "def get_X(df):\n",
    "    text_normalization(df) # Normalize text\n",
    "    remove_stop_words(df, language, get_tokenize = True) # Remove stop words [and Tokenize texts]\n",
    "\n",
    "    # Padding text\n",
    "    new_X = []\n",
    "    for seq in df.text:\n",
    "        new_seq = []\n",
    "        for i in range(max_length_sequence):\n",
    "            try:\n",
    "                new_seq.append(seq[i])\n",
    "            except:\n",
    "                new_seq.append(\"PADword\")\n",
    "        new_X.append(new_seq)\n",
    "\n",
    "    return(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jSoQcmYVtpDW"
   },
   "outputs": [],
   "source": [
    "max_length_sequence = 100\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9VnhCSwQt-_I"
   },
   "outputs": [],
   "source": [
    "X_english = get_X(english_dataset)\n",
    "X_translated = get_X(translated_dataset)\n",
    "\n",
    "Y_english = english_dataset.label.values\n",
    "Y_translated = translated_dataset.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlnsMixwu-tH",
    "outputId": "a7a850d8-af9c-44b1-afaa-4382a74d6a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6335 100\n",
      "2571 100\n"
     ]
    }
   ],
   "source": [
    "print(len(X_english), len(X_english[0]))\n",
    "print(len(X_translated), len(X_translated[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnStDw0HtZu0"
   },
   "source": [
    "Get ELMo from TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOrv3JQzsmyu",
    "outputId": "63b86377-3d44-4f35-baa7-b41237716c2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x164b3833288>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable = False)\n",
    "elmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuctFs9i-0tR"
   },
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3XV6I8tjtxfH"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ziSSsYiCwBFA"
   },
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_length_sequence])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JNTntyOFwHK0"
   },
   "outputs": [],
   "source": [
    "def create_model_RNN(max_length_sequence, lstm_units, l2_kernel, l2_recurrent, l2_activity, dropout):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ), dtype=tf.string)\n",
    "    \n",
    "    embedding_layer = Lambda(ElmoEmbedding, output_shape = (max_length_sequence, 1024))(X_input)\n",
    "    \n",
    "    X = LSTM(units = lstm_units, return_sequences = False,\n",
    "            kernel_regularizer = regularizers.l2(l2_kernel),\n",
    "            recurrent_regularizer = regularizers.l2(l2_recurrent),\n",
    "            activity_regularizer = regularizers.l2(l2_activity))(embedding_layer)\n",
    "    \n",
    "    X = Dropout(rate = dropout)(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FEmTHcQ4wNAp"
   },
   "outputs": [],
   "source": [
    "def create_model_CNN(max_length_sequence, filters, kernel_size, dense_units, l2_kernel):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ), dtype=tf.string)\n",
    "    \n",
    "    embedding_layer = Lambda(ElmoEmbedding, output_shape = (max_length_sequence, 1024))(X_input)\n",
    "    \n",
    "    X = Conv1D(filters = filters, kernel_size = kernel_size, activation = 'relu',\n",
    "              kernel_regularizer = regularizers.l2(l2_kernel))(embedding_layer)\n",
    "    X = MaxPooling1D(pool_size = 2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units = dense_units, activation = 'relu')(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0EzM3kQuOSI"
   },
   "source": [
    "Execute Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOA7KazguYqA"
   },
   "source": [
    "Train and Validation with English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lT-DwRKcw0T4"
   },
   "outputs": [],
   "source": [
    "epochs = 7\n",
    "test_size = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_english, Y_english, test_size = test_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2EN2pbIwPKI",
    "outputId": "bd8a82a4-2173-4b4e-cd40-f147daa29b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5701 5701 634 634\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw9_XDY27Vrd",
    "outputId": "01c75d25-a39f-4d18-acb3-cdaa670995ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696 5696\n",
      "608 608\n"
     ]
    }
   ],
   "source": [
    "fit_batch_size_train = int(len(X_train)/batch_size) * batch_size\n",
    "X_train = X_train[:fit_batch_size_train]\n",
    "Y_train = Y_train[:fit_batch_size_train]\n",
    "print(len(X_train), len(Y_train))\n",
    "\n",
    "fit_batch_size_test = int(len(X_test)/batch_size) * batch_size\n",
    "X_test = X_test[:fit_batch_size_test]\n",
    "Y_test = Y_test[:fit_batch_size_test]\n",
    "print(len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJmPTrw9ul-E"
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8V7rMdsrw6rY",
    "outputId": "4ffd0df1-a2fd-4d82-a499-4d7579ff5443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kevin\\anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kevin\\anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kevin\\anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kevin\\anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100, 1024)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 33056     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 33,065\n",
      "Trainable params: 33,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    RNN = create_model_RNN(max_length_sequence = max_length_sequence, lstm_units = 8, l2_kernel = 0, l2_recurrent = 0, l2_activity = 0, dropout = 0.5)\n",
    "    RNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    RNN.summary() \n",
    "    RNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    loss, acc = RNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "    print(loss, round(acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlbcQuOpuq7K"
   },
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G0xrDUgusG5",
    "outputId": "5e1dda1b-7b55-4fe5-fe40-6ab90ae68acb"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    CNN = create_model_CNN(max_length_sequence = max_length_sequence, filters = 16, kernel_size = 10, dense_units = 4, l2_kernel = 0)\n",
    "    CNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    CNN.summary() \n",
    "    CNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    loss, acc = CNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "    print(loss, round(acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2lGy9GputD_"
   },
   "source": [
    "Train with English Dataset and Validation with Translated Dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ELMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_tf1",
   "language": "python",
   "name": "env_tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
