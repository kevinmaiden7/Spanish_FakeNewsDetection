{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from data_preprocessing import remove_stop_words, text_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "L5a6emTisYeo",
    "outputId": "8773fd64-7cfe-42c1-c14f-80d918bcdfc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print (\"setting tensorflow version in colab\")\n",
    "    %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0Zg2jilsfC-",
    "outputId": "4754356a-786a-4e59-d1f2-5cc1e2037b5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyTRHmBZtquK",
    "outputId": "7068af51-b753-4c4f-ab49-ad2705e8909d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51233, 2) (2571, 2)\n"
     ]
    }
   ],
   "source": [
    "english_dataset= pd.read_csv('../data/Merged/english_dataset.csv')\n",
    "translated_dataset = pd.read_csv('../data/Merged/spanish_t_dataset.csv')\n",
    "print(english_dataset.shape, translated_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dGEd5xksyjqH"
   },
   "outputs": [],
   "source": [
    "def get_X(df):\n",
    "    text_normalization(df) # Normalize text\n",
    "    remove_stop_words(df, language, get_tokenize = True) # Remove stop words [and Tokenize texts]\n",
    "\n",
    "    # Padding text\n",
    "    new_X = []\n",
    "    for seq in df.text:\n",
    "        new_seq = []\n",
    "        for i in range(max_length_sequence):\n",
    "            try:\n",
    "                new_seq.append(seq[i])\n",
    "            except:\n",
    "                new_seq.append(\"PADword\")\n",
    "        new_X.append(new_seq)\n",
    "\n",
    "    return(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jSoQcmYVtpDW"
   },
   "outputs": [],
   "source": [
    "max_length_sequence = 50\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9VnhCSwQt-_I"
   },
   "outputs": [],
   "source": [
    "X_english = get_X(english_dataset)\n",
    "X_translated = get_X(translated_dataset)\n",
    "\n",
    "Y_english = english_dataset.label.values\n",
    "Y_translated = translated_dataset.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlnsMixwu-tH",
    "outputId": "a7a850d8-af9c-44b1-afaa-4382a74d6a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51233 50\n",
      "2571 50\n"
     ]
    }
   ],
   "source": [
    "print(len(X_english), len(X_english[0]))\n",
    "print(len(X_translated), len(X_translated[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnStDw0HtZu0"
   },
   "source": [
    "#### Get ELMo from TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOrv3JQzsmyu",
    "outputId": "63b86377-3d44-4f35-baa7-b41237716c2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x2df764f3b00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable = False)\n",
    "elmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuctFs9i-0tR"
   },
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3XV6I8tjtxfH"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ziSSsYiCwBFA"
   },
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_length_sequence])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JNTntyOFwHK0"
   },
   "outputs": [],
   "source": [
    "def create_model_RNN(max_length_sequence, lstm_units, l2_kernel, l2_recurrent, l2_activity, dropout):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ), dtype=tf.string)\n",
    "    \n",
    "    embedding_layer = Lambda(ElmoEmbedding, output_shape = (max_length_sequence, 1024))(X_input)\n",
    "    \n",
    "    X = LSTM(units = lstm_units, return_sequences = False,\n",
    "            kernel_regularizer = regularizers.l2(l2_kernel),\n",
    "            recurrent_regularizer = regularizers.l2(l2_recurrent),\n",
    "            activity_regularizer = regularizers.l2(l2_activity))(embedding_layer)\n",
    "    \n",
    "    X = Dropout(rate = dropout)(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FEmTHcQ4wNAp"
   },
   "outputs": [],
   "source": [
    "def create_model_CNN(max_length_sequence, filters, kernel_size, dense_units, l2_kernel):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ), dtype=tf.string)\n",
    "    \n",
    "    embedding_layer = Lambda(ElmoEmbedding, output_shape = (max_length_sequence, 1024))(X_input)\n",
    "    \n",
    "    X = Conv1D(filters = filters, kernel_size = kernel_size, activation = 'relu',\n",
    "              kernel_regularizer = regularizers.l2(l2_kernel))(embedding_layer)\n",
    "    X = MaxPooling1D(pool_size = 2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units = dense_units, activation = 'relu')(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0EzM3kQuOSI"
   },
   "source": [
    "#### Execute Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOA7KazguYqA"
   },
   "source": [
    "#### Train and Validation with English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw9_XDY27Vrd",
    "outputId": "01c75d25-a39f-4d18-acb3-cdaa670995ba"
   },
   "outputs": [],
   "source": [
    "def fit_batch_size(X_train, Y_train, X_test, Y_test):\n",
    "    fit_batch_size_train = int(len(X_train)/batch_size) * batch_size\n",
    "    X_train = X_train[:fit_batch_size_train]\n",
    "    Y_train = Y_train[:fit_batch_size_train]\n",
    "\n",
    "    fit_batch_size_test = int(len(X_test)/batch_size) * batch_size\n",
    "    X_test = X_test[:fit_batch_size_test]\n",
    "    Y_test = Y_test[:fit_batch_size_test]\n",
    "    \n",
    "    return(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lT-DwRKcw0T4"
   },
   "outputs": [],
   "source": [
    "epochs = 7\n",
    "test_size = 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_english, Y_english, test_size = test_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40960 40960 10240 10240\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = fit_batch_size(X_train, Y_train, X_test, Y_test)\n",
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJmPTrw9ul-E"
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8V7rMdsrw6rY",
    "outputId": "4ffd0df1-a2fd-4d82-a499-4d7579ff5443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8)                 33056     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 33,065\n",
      "Trainable params: 33,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "40960/40960 [==============================] - 5346s 131ms/step - loss: 0.2369 - accuracy: 0.9170\n",
      "Epoch 2/7\n",
      "40960/40960 [==============================] - 5291s 129ms/step - loss: 0.1315 - accuracy: 0.9584\n",
      "Epoch 3/7\n",
      "40960/40960 [==============================] - 5299s 129ms/step - loss: 0.1056 - accuracy: 0.9664\n",
      "Epoch 4/7\n",
      "40960/40960 [==============================] - 5340s 130ms/step - loss: 0.0911 - accuracy: 0.9719\n",
      "Epoch 5/7\n",
      "40960/40960 [==============================] - 5309s 130ms/step - loss: 0.0803 - accuracy: 0.9745\n",
      "Epoch 6/7\n",
      "40960/40960 [==============================] - 5338s 130ms/step - loss: 0.0723 - accuracy: 0.9777\n",
      "Epoch 7/7\n",
      "40960/40960 [==============================] - 5336s 130ms/step - loss: 0.0635 - accuracy: 0.9804\n",
      "10240/10240 [==============================] - 1459s 142ms/step\n",
      "0.08423896026215516 0.973\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    RNN = create_model_RNN(max_length_sequence = max_length_sequence, lstm_units = 8, l2_kernel = 0, l2_recurrent = 0, l2_activity = 0, dropout = 0.5)\n",
    "    RNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    RNN.summary() \n",
    "    RNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    loss, acc = RNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "    print(loss, round(acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlbcQuOpuq7K"
   },
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G0xrDUgusG5",
    "outputId": "5e1dda1b-7b55-4fe5-fe40-6ab90ae68acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 41, 16)            163856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1284      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 165,145\n",
      "Trainable params: 165,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40960/40960 [==============================] - 5829s 142ms/step - loss: 0.3257 - accuracy: 0.9267\n",
      "Epoch 2/5\n",
      "40960/40960 [==============================] - 5730s 140ms/step - loss: 0.1959 - accuracy: 0.9499\n",
      "Epoch 3/5\n",
      "40960/40960 [==============================] - 5753s 140ms/step - loss: 0.1388 - accuracy: 0.9626\n",
      "Epoch 4/5\n",
      "40960/40960 [==============================] - 5640s 138ms/step - loss: 0.1076 - accuracy: 0.9710\n",
      "Epoch 5/5\n",
      "40960/40960 [==============================] - 5479s 134ms/step - loss: 0.0869 - accuracy: 0.9781\n",
      "10240/10240 [==============================] - 1330s 130ms/step\n",
      "0.1422757276566699 0.957\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    CNN = create_model_CNN(max_length_sequence = max_length_sequence, filters = 16, kernel_size = 10, dense_units = 4, l2_kernel = 0)\n",
    "    CNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    CNN.summary() \n",
    "    CNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    loss, acc = CNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "    print(loss, round(acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2lGy9GputD_"
   },
   "source": [
    "#### Train with English Dataset and Validation with Translated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_english\n",
    "Y_train = Y_english\n",
    "X_test = X_translated\n",
    "Y_test = Y_translated\n",
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51232 51232 2560 2560\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = fit_batch_size(X_train, Y_train, X_test, Y_test)\n",
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 41, 16)            163856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1284      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 165,145\n",
      "Trainable params: 165,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "51232/51232 [==============================] - 7225s 141ms/step - loss: 0.1484 - accuracy: 0.9433\n",
      "Epoch 2/7\n",
      "51232/51232 [==============================] - 7276s 142ms/step - loss: 0.0891 - accuracy: 0.9678\n",
      "Epoch 3/7\n",
      "51232/51232 [==============================] - 6978s 136ms/step - loss: 0.0635 - accuracy: 0.9763\n",
      "Epoch 4/7\n",
      "51232/51232 [==============================] - 6639s 130ms/step - loss: 0.0453 - accuracy: 0.9825\n",
      "Epoch 5/7\n",
      "51232/51232 [==============================] - 6646s 130ms/step - loss: 0.0329 - accuracy: 0.9878\n",
      "Epoch 6/7\n",
      "51232/51232 [==============================] - 6619s 129ms/step - loss: 0.0261 - accuracy: 0.9901\n",
      "Epoch 7/7\n",
      "51232/51232 [==============================] - 6661s 130ms/step - loss: 0.0192 - accuracy: 0.9929\n",
      "2560/2560 [==============================] - 328s 128ms/step\n",
      "2.991 0.525\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    CNN = create_model_CNN(max_length_sequence = max_length_sequence, filters = 16, kernel_size = 10, dense_units = 4, l2_kernel = 0)\n",
    "    CNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    CNN.summary()\n",
    "    CNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    loss, acc = CNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "    print(round(loss, 3), round(acc, 3))\n",
    "    CNN.save_weights('../data/Weights/ELMo_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train with English Dataset and Validation with Translated Dataset - Mixing 2500 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add n_samples from data_2 to data_1\n",
    "def add_data_portion(data_1, data_2, n_samples):\n",
    "    df_slice = data_2.sample(n_samples)\n",
    "    df_rest = data_2.loc[~data_2.index.isin(df_slice.index)]\n",
    "    df_extended = data_1.append(df_slice, ignore_index = True)\n",
    "    \n",
    "    df_rest.reset_index(inplace = True)\n",
    "    del(df_rest['index'])\n",
    "    df_extended.reset_index(inplace = True)\n",
    "    del(df_extended['index'])\n",
    "    \n",
    "    return df_extended, df_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51233, 2) (2571, 2)\n"
     ]
    }
   ],
   "source": [
    "english_dataset= pd.read_csv('../data/Merged/english_dataset.csv')\n",
    "translated_dataset = pd.read_csv('../data/Merged/spanish_t_dataset.csv')\n",
    "print(english_dataset.shape, translated_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53733, 2) (71, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = add_data_portion(english_dataset, translated_dataset, 2500) # add 2500 samples from data_2 to data_1\n",
    "print(train_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalvarez\\projects\\Spanish_FakeNewsDetection\\src\\data_preprocessing.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: x.lower())\n",
      "C:\\Users\\aalvarez\\projects\\Spanish_FakeNewsDetection\\src\\data_preprocessing.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply((lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x)))\n"
     ]
    }
   ],
   "source": [
    "X_train = get_X(train_dataset)\n",
    "X_test = get_X(test_dataset)\n",
    "\n",
    "Y_train = train_dataset.label.values\n",
    "Y_test = test_dataset.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53733 50\n",
      "71 50\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train[0]))\n",
    "print(len(X_test), len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53728 53728 64 64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = fit_batch_size(X_train, Y_train, X_test, Y_test)\n",
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 50, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 41, 16)            163856    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1284      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 165,145\n",
      "Trainable params: 165,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aalvarez\\Anaconda3\\envs\\env_tf1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "53728/53728 [==============================] - 6825s 127ms/step - loss: 0.1924 - accuracy: 0.9233\n",
      "Epoch 2/7\n",
      "53728/53728 [==============================] - 6993s 130ms/step - loss: 0.1182 - accuracy: 0.9534\n",
      "Epoch 3/7\n",
      "53728/53728 [==============================] - 7062s 131ms/step - loss: 0.0810 - accuracy: 0.9685\n",
      "Epoch 4/7\n",
      "53728/53728 [==============================] - 6901s 128ms/step - loss: 0.0537 - accuracy: 0.9800\n",
      "Epoch 5/7\n",
      "53728/53728 [==============================] - 6787s 126ms/step - loss: 0.0352 - accuracy: 0.9867\n",
      "Epoch 6/7\n",
      "53728/53728 [==============================] - 6739s 125ms/step - loss: 0.0270 - accuracy: 0.9895\n",
      "Epoch 7/7\n",
      "53728/53728 [==============================] - 7929s 148ms/step - loss: 0.0203 - accuracy: 0.9930\n",
      "64/64 [==============================] - 11s 168ms/step\n",
      "1.283 0.703\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    CNN = create_model_CNN(max_length_sequence = max_length_sequence, filters = 16, kernel_size = 10, dense_units = 4, l2_kernel = 0)\n",
    "    CNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "    CNN.summary()\n",
    "    CNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "    loss, acc = CNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "    print(round(loss, 3), round(acc, 3))\n",
    "    CNN.save_weights('../data/Weights/ELMo_CNN_Plus_2500.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ELMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_tf1",
   "language": "python",
   "name": "env_tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
