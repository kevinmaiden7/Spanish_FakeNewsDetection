{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "L5a6emTisYeo",
    "outputId": "b32579e2-7b36-4013-9fa4-b01024f1f7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting tensorflow version in colab\n",
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.15.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print (\"setting tensorflow version in colab\")\n",
    "    %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0Zg2jilsfC-",
    "outputId": "134616eb-faa3-4d31-a5d0-3b3ae893f9dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from data_preprocessing import remove_stop_words, text_normalization\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyTRHmBZtquK",
    "outputId": "eb13cfc9-e411-443a-fe0a-7269aa59a9a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 2) (2571, 2)\n"
     ]
    }
   ],
   "source": [
    "english_dataset = pd.read_csv('small_english_dataset.csv')\n",
    "translated_dataset = pd.read_csv('spanish_t_dataset.csv')\n",
    "print(english_dataset.shape, translated_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dGEd5xksyjqH"
   },
   "outputs": [],
   "source": [
    "def get_X(df):\n",
    "  text_normalization(df) # Normalize text\n",
    "  remove_stop_words(df, language, get_tokenize = True) # Remove stop words [and Tokenize texts]\n",
    "\n",
    "  # Padding text\n",
    "  new_X = []\n",
    "  for seq in df.text:\n",
    "      new_seq = []\n",
    "      for i in range(max_length_sequence):\n",
    "          try:\n",
    "              new_seq.append(seq[i])\n",
    "          except:\n",
    "              new_seq.append(\"PADword\")\n",
    "      new_X.append(new_seq)\n",
    "\n",
    "  return(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jSoQcmYVtpDW"
   },
   "outputs": [],
   "source": [
    "max_length_sequence = 100\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9VnhCSwQt-_I"
   },
   "outputs": [],
   "source": [
    "X_english = get_X(english_dataset)\n",
    "X_translated = get_X(translated_dataset)\n",
    "\n",
    "Y_english = english_dataset.label.values\n",
    "Y_translated = translated_dataset.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlnsMixwu-tH",
    "outputId": "f4c81652-5422-4ec6-de45-bd689399baab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6335 100\n",
      "2571 100\n"
     ]
    }
   ],
   "source": [
    "print(len(X_english), len(X_english[0]))\n",
    "print(len(X_translated), len(X_translated[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnStDw0HtZu0"
   },
   "source": [
    "Get ELMo from TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOrv3JQzsmyu",
    "outputId": "3bedd7e9-9cbf-40c5-b021-5598d064a879"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x7f4b62ba5940>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable = False)\n",
    "elmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuctFs9i-0tR"
   },
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3XV6I8tjtxfH"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ziSSsYiCwBFA"
   },
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_length_sequence])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JNTntyOFwHK0"
   },
   "outputs": [],
   "source": [
    "def create_model_RNN(max_length_sequence, lstm_units, l2_kernel, l2_recurrent, l2_activity, dropout):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ), dtype=tf.string)\n",
    "    \n",
    "    embedding_layer = Lambda(ElmoEmbedding, output_shape = (max_length_sequence, 1024))(X_input)\n",
    "    \n",
    "    X = LSTM(units = lstm_units, return_sequences = False,\n",
    "            kernel_regularizer = regularizers.l2(l2_kernel),\n",
    "            recurrent_regularizer = regularizers.l2(l2_recurrent),\n",
    "            activity_regularizer = regularizers.l2(l2_activity))(embedding_layer)\n",
    "    \n",
    "    X = Dropout(rate = dropout)(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FEmTHcQ4wNAp"
   },
   "outputs": [],
   "source": [
    "def create_model_CNN(max_length_sequence, filters, kernel_size, dense_units, l2_kernel):\n",
    "    \n",
    "    X_input = Input(shape = (max_length_sequence, ), dtype=tf.string)\n",
    "    \n",
    "    embedding_layer = Lambda(ElmoEmbedding, output_shape = (max_length_sequence, 1024))(X_input)\n",
    "    \n",
    "    X = Conv1D(filters = filters, kernel_size = kernel_size, activation = 'relu',\n",
    "              kernel_regularizer = regularizers.l2(l2_kernel))(embedding_layer)\n",
    "    X = MaxPooling1D(pool_size = 2)(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(units = dense_units, activation = 'relu')(X)\n",
    "    X = Dense(units = 1, activation = 'sigmoid')(X)\n",
    "                          \n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "                          \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0EzM3kQuOSI"
   },
   "source": [
    "Execute Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOA7KazguYqA"
   },
   "source": [
    "Train and Validation with English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lT-DwRKcw0T4"
   },
   "outputs": [],
   "source": [
    "epochs = 7\n",
    "test_size = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_english, Y_english, test_size = test_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2EN2pbIwPKI",
    "outputId": "95a0d0b2-fd7e-42fe-c801-b6f50d464d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5701 634\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uw9_XDY27Vrd",
    "outputId": "3d9c9e85-97e4-4d6d-b264-8a825677126c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696 5696\n",
      "608 608\n"
     ]
    }
   ],
   "source": [
    "fit_batch_size_train = int(len(X_train)/batch_size) * batch_size\n",
    "X_train = X_train[:fit_batch_size_train]\n",
    "Y_train = Y_train[:fit_batch_size_train]\n",
    "print(len(X_train), len(Y_train))\n",
    "\n",
    "fit_batch_size_test = int(len(X_test)/batch_size) * batch_size\n",
    "X_test = X_train[:fit_batch_size_test]\n",
    "Y_test = Y_train[:fit_batch_size_test]\n",
    "print(len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJmPTrw9ul-E"
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8V7rMdsrw6rY",
    "outputId": "7619b4d1-9c0d-4364-f864-57d5f65b2acd"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  RNN = create_model_RNN(max_length_sequence = max_length_sequence, lstm_units = 8, l2_kernel = 0, l2_recurrent = 0, l2_activity = 0, dropout = 0.5)\n",
    "  RNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "  RNN.summary() \n",
    "  RNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "  loss, acc = RNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "  print(loss, round(acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlbcQuOpuq7K"
   },
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G0xrDUgusG5",
    "outputId": "e2a8fa00-4ff2-42b1-e27d-6a97a8841d65"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  CNN = create_model_CNN(max_length_sequence = max_length_sequence, filters = 16, kernel_size = 10, dense_units = 4, l2_kernel = 0)\n",
    "  CNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Compile model\n",
    "  CNN.summary() \n",
    "  CNN.fit(np.array(X_train), Y_train, epochs = epochs, batch_size = batch_size, shuffle = True) # Fit model\n",
    "  loss, acc = CNN.evaluate(np.array(X_test), Y_test) # Evaluate model\n",
    "  print(loss, round(acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2lGy9GputD_"
   },
   "source": [
    "Train with English Dataset and Validation with Translated Dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ELMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
