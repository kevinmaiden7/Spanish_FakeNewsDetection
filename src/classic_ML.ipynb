{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from data_preprocessing import get_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(model, parameters, dataset, X):\n",
    "    \n",
    "    Y = dataset.label.values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)\n",
    "    \n",
    "    iterations = 5\n",
    "    index = ShuffleSplit(n_splits = iterations, test_size = 0.2)\n",
    "    \n",
    "    clf = GridSearchCV(model, parameters, cv = index, scoring = 'accuracy')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = clf.best_params_\n",
    "    best_score_validation = clf.best_score_\n",
    "\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, Y_pred)\n",
    "    \n",
    "    return best_model, best_score_validation, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>La palabra \"haiga\", aceptada por la RAE La Rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>UNAM capacitará a maestros para aprobar prueba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Alerta: pretenden aprobar libros escolares con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2566</td>\n",
       "      <td>Recuperamos la historia de Aleixandra, la jove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2567</td>\n",
       "      <td>Reproches, tensión y sinceridad: la comida en ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2568</td>\n",
       "      <td>RT @ElMundoOpinion: \"PSOE, PP, Ciudadanos y Vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2569</td>\n",
       "      <td>Rusia cita al embajador español por unas decla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>Saeed Malekpour fue detenido en 2008, cuando v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...      1\n",
       "1     La palabra \"haiga\", aceptada por la RAE La Rea...      1\n",
       "2     YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...      1\n",
       "3     UNAM capacitará a maestros para aprobar prueba...      0\n",
       "4     Alerta: pretenden aprobar libros escolares con...      1\n",
       "...                                                 ...    ...\n",
       "2566  Recuperamos la historia de Aleixandra, la jove...      0\n",
       "2567  Reproches, tensión y sinceridad: la comida en ...      0\n",
       "2568  RT @ElMundoOpinion: \"PSOE, PP, Ciudadanos y Vo...      0\n",
       "2569  Rusia cita al embajador español por unas decla...      0\n",
       "2570  Saeed Malekpour fue detenido en 2008, cuando v...      0\n",
       "\n",
       "[2571 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/Merged/spanish_dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2571, 5000)\n",
      "(2571, 5000)\n"
     ]
    }
   ],
   "source": [
    "matrix_1, df = get_matrix(data = dataset, representation = 'BoW', vocabulary_length = 5000, stemming = False, remove_stopwords = False, language = 'spanish')\n",
    "matrix_2, df = get_matrix(data = dataset, representation = 'tf-idf', vocabulary_length = 5000, stemming = False, remove_stopwords = False, language = 'spanish')\n",
    "print(matrix_1.shape)\n",
    "print(matrix_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF = RandomForestClassifier()\n",
    "model_GBT = GradientBoostingClassifier()\n",
    "parameters_RF_GBT = {'n_estimators': [50, 100, 200, 300], \n",
    "                     'max_features': [50, 100, 200, 300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SVC = SVC()\n",
    "parameters_SVC = {'kernel': ['linear', 'rbf'],\n",
    "                  'C': [1e3, 1, 0.001],\n",
    "                  'gamma': [0.1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLPC = MLPClassifier(activation = 'relu', solver = 'adam')\n",
    "parameters_MLPC = {'hidden_layer_sizes': [(10),(50),(10,10),(50,50),(10,10,10),(50,50,50)],\n",
    "                   'max_iter': [1000,1500]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix_1 = BoW; matrix_2 = tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RF')\n",
    "print('BoW', grid_search_cv(model_RF, parameters_RF_GBT, df, matrix_1))\n",
    "print('tf-idf', grid_search_cv(model_RF, parameters_RF_GBT, df, matrix_2))\n",
    "print('GBT')\n",
    "print('BoW', grid_search_cv(model_GBT, parameters_RF_GBT, df, matrix_1))\n",
    "print('tf-idf', grid_search_cv(model_GBT, parameters_RF_GBT, df, matrix_2))\n",
    "print('SVC')\n",
    "print('BoW', grid_search_cv(model_SVC, parameters_SVC, df, matrix_1))\n",
    "print('tf-idf', grid_search_cv(model_SVC, parameters_SVC, df, matrix_2))\n",
    "print('MLPC')\n",
    "print('BoW', grid_search_cv(model_MLPC, parameters_MLPC, df, matrix_1))\n",
    "print('tf-idf', grid_search_cv(model_MLPC, parameters_MLPC, df, matrix_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_step_2(model, parameters, representation):\n",
    "    dictionary = [10e3, 20e3, 30e3, 40e3]\n",
    "    stemming = [False, True]\n",
    "    stop_words = [False, True]\n",
    "    for i in range(np.size(dictionary)):\n",
    "        for j in range(2):\n",
    "            for k in range(2):\n",
    "                matrix, df = get_matrix(dataset, representation = representation, \n",
    "                                        vocabulary_length = dictionary[i], stemming = stemming[j], remove_stopwords = stop_words[k],  language = 'spanish')\n",
    "                print(grid_search_cv(model, parameters, df, matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
