{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length_text(df):\n",
    "    max_length = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        length = np.size(word_tokenize(df.at[i, 'text']))\n",
    "        if length > max_length: max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(data):\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "    data['text'] = data['text'].apply((lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, language, get_tokenize):\n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    if get_tokenize:\n",
    "        for i in range(data.shape[0]):\n",
    "            data.at[i, 'text'] = [word for word in nltk.word_tokenize(data.at[i, 'text']) if word not in stopwords]\n",
    "    else:\n",
    "        for i in range(data.shape[0]):\n",
    "            data.at[i, 'text'] = [word for word in nltk.word_tokenize(data.at[i, 'text']) if word not in stopwords]\n",
    "            data.at[i, 'text'] = ' '.join(data.at[i, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(data, language):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    for i in range(data.shape[0]):\n",
    "         data.at[i, 'text'] = (' '.join([stemmer.stem(word) for word in data.at[i, 'text'].split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_matrix representation | BoW and Tf-idf for Classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data, representation, vocabulary_length, stemming, remove_stopwords, language):\n",
    "\n",
    "    df = data.copy(deep = True)\n",
    "    \n",
    "    text_normalization(df) # Text normalization\n",
    "    \n",
    "    # Stop_words\n",
    "    if remove_stopwords:\n",
    "        remove_stop_words(df, language, False)\n",
    "    \n",
    "    # Stemming\n",
    "    if stemming:\n",
    "        apply_stemming(df, language)\n",
    "    \n",
    "    # Word representation\n",
    "    if representation == 'BoW':\n",
    "        count_vectorizer = CountVectorizer(max_df = 0.9, max_features = vocabulary_length, min_df = 0)\n",
    "        #count_vectorizer = CountVectorizer(max_features = vocabulary_length)\n",
    "        matrix = count_vectorizer.fit_transform(df.text)\n",
    "        \n",
    "    elif representation == 'tf-idf':\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_df = 0.9, max_features = vocabulary_length, min_df = 0, use_idf = True)\n",
    "        #tfidf_vectorizer = TfidfVectorizer(max_features = vocabulary_length, use_idf=True)\n",
    "        matrix = tfidf_vectorizer.fit_transform(df.text)\n",
    "    \n",
    "    return matrix, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing for RNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_RNN(data, stemming, remove_stopwords, vocabulary_length, max_length_sequence, language):\n",
    "    \n",
    "    df = data.copy(deep = True)\n",
    "    \n",
    "    text_normalization(df) # Text normalization\n",
    "    \n",
    "    # Stemming\n",
    "    if stemming:\n",
    "        apply_stemming(df, language)\n",
    "    \n",
    "    # Stop_words\n",
    "    if remove_stopwords:\n",
    "        remove_stop_words(df, language, True)\n",
    "        \n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(num_words = vocabulary_length)\n",
    "    tokenizer.fit_on_texts(df.text)\n",
    "    X = tokenizer.texts_to_sequences(df.text)\n",
    "    \n",
    "    # Padding\n",
    "    X = pad_sequences(X, maxlen = max_length_sequence, padding = 'post', truncating = 'post')\n",
    "    \n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>La palabra \"haiga\", aceptada por la RAE La Rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>UNAM capacitará a maestros para aprobar prueba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Alerta: pretenden aprobar libros escolares con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2566</td>\n",
       "      <td>Recuperamos la historia de Aleixandra, la jove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2567</td>\n",
       "      <td>Reproches, tensión y sinceridad: la comida en ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2568</td>\n",
       "      <td>RT @ElMundoOpinion: \"PSOE, PP, Ciudadanos y Vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2569</td>\n",
       "      <td>Rusia cita al embajador español por unas decla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>Saeed Malekpour fue detenido en 2008, cuando v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...      1\n",
       "1     La palabra \"haiga\", aceptada por la RAE La Rea...      1\n",
       "2     YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...      1\n",
       "3     UNAM capacitará a maestros para aprobar prueba...      0\n",
       "4     Alerta: pretenden aprobar libros escolares con...      1\n",
       "...                                                 ...    ...\n",
       "2566  Recuperamos la historia de Aleixandra, la jove...      0\n",
       "2567  Reproches, tensión y sinceridad: la comida en ...      0\n",
       "2568  RT @ElMundoOpinion: \"PSOE, PP, Ciudadanos y Vo...      0\n",
       "2569  Rusia cita al embajador español por unas decla...      0\n",
       "2570  Saeed Malekpour fue detenido en 2008, cuando v...      0\n",
       "\n",
       "[2571 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/Merged/spanish_dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2865"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_text(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONARIO DEL IDIOMA ESPAÑOL COMO DEFINICIÓN DE \"MUJER PROBLEMÁTICA\"\n",
      "España.- El presidente de la Real Academia Española (RAE), Darío Villanueva, informó en conferencia de prensa que a partir del próximo mes se incluirá el término \"Lady\" como una nueva palabra en el diccionario del idioma español.\n",
      "Darío señaló que \"Lady\" servirá para definir a una \"mujer problemática\" o a una \"mujer que causa problemas\", y mencionó que esta palabra será una de las pocas que también se utilizan en el idioma inglés pero que en castellano tiene un significado diferente:\n",
      "\"Son contadas las palabras del idioma inglés que se utilizan en el español pero que tienen otro significado. Con la globalización las personas han comenzado a adoptar términos anglosajones pero los utilizan con su significado real, sin embargo en este caso la expresión Lady no significará lo mismo que en su idioma original (\"dama\" en inglés) sino que se usará para definir a una mujer que es problemática o acostumbra causar problemas y alborotos.\n",
      "La gente podrá decirle Lady a una fémina que cause algún escándalo, sea agresiva o provoque algún tipo de problema. El término dejara de considerarse una palabra exclusiva del idioma inglés ya que tras se incluida en el diccionario de la lengua española también pasara ser una palabra oficial del castellano, pero con un significado distinto\", confesó.\n",
      "Villanueva presentó a los medios la definición oficial que aparecerá en los diccionarios, señalando que será la siguiente:\n",
      "-Lady\n",
      "Del anglosajón Ænglisc, part. de Difficilis 'problematica', ferox 'agresiva'\n",
      "*NUMBER*.- adj. f. Mujer excesivamente problemática\n",
      "*NUMBER*.- adj. f. Mujer que causa problemas o alborotos\n",
      "*NUMBER*.- adj. f. Mujer que tiende a causar conflictos, es agresiva\n",
      "Te puede interesar Cholas descubren que las Donitas Bimbo también se pueden comer y no solo sirven para maquillarse\n",
      "*NUMBER*.- adj. f. Mujer que se guía por sus instintos animales, que no le importa crear conflictos\n",
      "El presidente señaló que fue uno de los miembros mexicanos de la RAE quien propuso incluir la palabra, y tras meses de análisis finamente fue aceptada por el comité:\n",
      "\"Es una término que tuvo su origen en México pero se usará en todos los países de habla hispana. Los videos de las Lady´s que han circulado nos sirvieron para crear una perfecta definición, la cual servirá para resumir y definir a una hembra problemática\", dijo\n",
      "Por último, Darío reveló que también ya se encuentran analizando la idea de incluir el término Lord en el diccionario, que sería el equivalente a la definición masculina de Lady.\n"
     ]
    }
   ],
   "source": [
    "print(dataset.at[0, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rae', 'inclu', 'palabr', 'lady', 'diccionari', 'idiom', 'español', 'com', 'definicion', 'muj', 'problemat', 'españ', 'president', 'real', 'academi', 'español', 'rae', 'dari', 'villanuev', 'inform', 'conferent', 'prens', 'part', 'proxim', 'mes', 'inclu', 'termin', 'lady', 'com', 'nuev', 'palabr', 'diccionari', 'idiom', 'español', 'dari', 'señal', 'lady', 'serv', 'par', 'defin', 'muj', 'problemat', 'muj', 'caus', 'problem', 'mencion', 'palabr', 'ser', 'poc', 'tambien', 'utiliz', 'idiom', 'ingles', 'per', 'castellan', 'tien', 'signific', 'diferent', 'cont', 'palabr', 'idiom', 'ingles', 'utiliz', 'español', 'per', 'tien', 'signific', 'globaliz', 'person', 'comenz', 'adopt', 'termin', 'anglosajon', 'per', 'utiliz', 'signific', 'real', 'embarg', 'cas', 'expresion', 'lady', 'signific', 'mism', 'idiom', 'original', 'dam', 'ingles', 'usar', 'par', 'defin', 'muj', 'problemat', 'acostumbr', 'caus', 'problem', 'alborot', 'gent', 'podr', 'dec', 'lady', 'femin', 'caus', 'algun', 'escandal', 'agres', 'provoqu', 'algun', 'tip', 'problem', 'termin', 'dej', 'consider', 'palabr', 'exclus', 'idiom', 'ingles', 'tras', 'inclu', 'diccionari', 'lengu', 'español', 'tambien', 'pas', 'ser', 'palabr', 'oficial', 'castellan', 'per', 'signific', 'distint', 'confes', 'villanuev', 'present', 'medi', 'definicion', 'oficial', 'aparec', 'diccionari', 'señal', 'ser', 'siguient', 'lady', 'anglosajon', 'ænglisc', 'part', 'difficilis', 'problemat', 'ferox', 'agres', 'numb', 'adj', 'f', 'muj', 'exces', 'problemat', 'numb', 'adj', 'f', 'muj', 'caus', 'problem', 'alborot', 'numb', 'adj', 'f', 'muj', 'tiend', 'caus', 'conflict', 'agres', 'pued', 'interes', 'chol', 'descubr', 'donit', 'bimb', 'tambien', 'pued', 'com', 'sol', 'sirv', 'par', 'maquill', 'numb', 'adj', 'f', 'muj', 'gui', 'instint', 'animal', 'import', 'cre', 'conflict', 'president', 'señal', 'miembr', 'mexican', 'rae', 'qui', 'propus', 'inclu', 'palabr', 'tras', 'mes', 'analisis', 'fin', 'acept', 'comit', 'termin', 'tuv', 'orig', 'mexic', 'per', 'usar', 'tod', 'pais', 'habl', 'hispan', 'vide', 'lady´s', 'circul', 'sirv', 'par', 'cre', 'perfect', 'definicion', 'serv', 'par', 'resum', 'defin', 'hembr', 'problemat', 'dij', 'ultim', 'dari', 'revel', 'tambien', 'encuentr', 'analiz', 'ide', 'inclu', 'termin', 'lord', 'diccionari', 'ser', 'equivalent', 'definicion', 'masculin', 'lady']\n",
      "(2571, 2)\n"
     ]
    }
   ],
   "source": [
    "matrix, df = get_input_RNN(dataset, True, True, 40000, 2900, language = 'spanish')\n",
    "print(df.at[0, 'text'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2571\n",
      "[1538  235  422 ...    0    0    0]\n",
      "2900\n"
     ]
    }
   ],
   "source": [
    "print(len(matrix))\n",
    "print(matrix[0])\n",
    "print(len(matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 422 5516  368 ...    0    0    0]\n",
      "2900\n"
     ]
    }
   ],
   "source": [
    "print(matrix[1])\n",
    "print(len(matrix[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
